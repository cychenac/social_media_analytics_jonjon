{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqVAFI26_YYn"
   },
   "source": [
    "# Lecture 6 - Sentiment Analysis\n",
    "\n",
    "In this notebook we will learn how to measure sentiment in text. \n",
    "\n",
    "Below is the overview of this notebook.\n",
    "<ol type = 1>\n",
    "<li> Measure tweet sentiment</li>\n",
    "<ol type = a>\n",
    "<li> Load tweets from database</li>\n",
    "<li> Load sentiment classifier from huggingface, which in this case is a BERT transformer</li>\n",
    "<li>  Measure tweet sentiment with transformer</li>\n",
    "<li> Calculate transformer embedding of tweet</li>\n",
    "\n",
    "</ol>\n",
    "\n",
    "<li> Analyze sentiment of tweets </li>\n",
    "<ol type = a>\n",
    "\n",
    "<li> Plot sentiment versus retweet counts</li>\n",
    "<li> Visualize UMAP transformer tweet embeddings</li>\n",
    "</ol>\n",
    "</ol>\n",
    "\n",
    "Below are some cool blogs you can read to learn more about the BERT transformer.\n",
    "\n",
    "http://jalammar.github.io/illustrated-bert/\n",
    "\n",
    "https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77\n",
    "\n",
    "https://github.com/jessevig/bertviz\n",
    "\n",
    "This notebook can be opened in Colab \n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zlisto/social_media_analytics/blob/main/Lecture06_SentimentAnalysis.ipynb)\n",
    "\n",
    "Before starting, select \"Runtime->Factory reset runtime\" to start with your directories and environment in the base state.\n",
    "\n",
    "If you want to save changes to the notebook, select \"File->Save a copy in Drive\" from the top menu in Colab.  This will save the notebook in your Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2GE_isIEXvU"
   },
   "source": [
    "# Clones, installs, imports, and GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9Fc7fpj9l1E"
   },
   "source": [
    "## Using a GPU\n",
    "\n",
    "If we switch the run-time to a GPU (graphical processing unit), the neural network computations will run faster.  To do this, go to the top left menu and select **Runtime-> Change runtime type -> Harware accelerator -> GPU**.  \n",
    "\n",
    "The code below will tell you if your Colab runtime is using a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fPJsV5769_Ke"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLUoNuR7_vQF"
   },
   "source": [
    "## Clone GitHub Repository\n",
    "This will clone the repository to your machine.  This includes the code and data files.  Then change into the directory of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PB60QQl_wIU"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/zlisto/social_media_analytics\n",
    "\n",
    "import os\n",
    "os.chdir(\"social_media_analytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dbev7JQa_YYs"
   },
   "source": [
    "## Install Requirements \n",
    "\n",
    "\n",
    "The main package we need today is `transformers` - this lets us use pre-trained transformer models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xS1qHpXn_YYt"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4yIPL9I_YYv"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQPaNehQ_YYw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import umap\n",
    "import scripts.TextAnalysis as ta\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import codecs  #this let's us display tweets properly (emojis, etc.)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f620FsVv_YYz"
   },
   "source": [
    "# Sentiment Classification with BERT\n",
    "\n",
    "We will pass the tweets through a pre-trained sentiment classifier with a BERT core.  Then we will plot the tweets with UMAP and color them by their sentiment.  Hopefully the positive and negative are in different regions of the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdw9yGST_YY0"
   },
   "source": [
    "### Download Pre-Traine Model and Tokenizer\n",
    "\n",
    "We will download the model and tokenizer from https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment.  This is a pre-trained model in the huggingface library that was trained on product reviews in multiple languages.  The output sentiment is between 1 and 5.\n",
    "\n",
    "There are many other models on huggingface that you can find here: https://huggingface.co/models?pipeline_tag=text-classification.\n",
    "\n",
    "We will create a tokenizer for the model called `tokenizer` and create the model itself, which we call `model`.  Every model needs its own tokenizer which tells it how to map text into the proper input vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbdXP2CZzs_b"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTU47aZ1_YY0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj7km1Qp_YY1"
   },
   "source": [
    "### Define Sentiment Classifier and Transformer Embedding Function\n",
    "\n",
    "When we pass text through our transformer model, we get many pieces of data in the output. First, we get the sentiment of the text.  Second, we get the text embedding in the final layer of the transformer.  Remember, inside the transformer we turn the input text into a high dimensional vector. This is the transformer embedding, and it is designed to separate text based on sentiment.\n",
    "\n",
    "\n",
    "We will create a function called `sentiment_classifier` which takes as input a string `text`, a transformer model called `model`, and a tokenizer called `tokenizer`, and returns the `sentiment` and `embedding`of the text.  The raw sentiment output of the model is a probability for each sentiment value.  The function will return the average sentiment based on these probabilities.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBAefiej_YY1"
   },
   "outputs": [],
   "source": [
    "def sentiment_classifier(text,model,tokenizer):\n",
    "    inputs = tokenizer.encode_plus(text, return_tensors='pt', add_special_tokens=True)\n",
    "\n",
    "    token_type_ids = inputs['token_type_ids']\n",
    "    input_ids = inputs['input_ids']\n",
    "\n",
    "    output = model(input_ids, token_type_ids=token_type_ids,return_dict=True,output_hidden_states=True)\n",
    "    logits = np.array(output.logits.tolist()[0])\n",
    "    prob = np.exp(logits)/np.sum(np.exp(logits))\n",
    "    sentiment = np.sum([(x+1)*prob[x] for x in range(len(prob))])  #use this line if you want the mean score\n",
    "    embedding = output.hidden_states[12].detach().numpy().squeeze()[0]\n",
    "    \n",
    "    return sentiment,embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o20i4iWX_YY2"
   },
   "source": [
    "### Sentiment Classification Example\n",
    "Now we can test the model on some text.  Feel free to try any text you want here.  Just put your text in the list `Text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gddIDn1R_YY2"
   },
   "outputs": [],
   "source": [
    "Text = [\"This class is kinda boring, but informative\", \n",
    "        \"This class is awesome\",\n",
    "        \"this class is stupid\",         \n",
    "        \"this class is dope\", \n",
    "       \"this class is crazy hard\",\n",
    "       \"this class is fun\",\n",
    "       \"this class is fun!\",\n",
    "       \"this class is :(\",\n",
    "       \"this class is :)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtnUiYmGzvnR"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "EU5-rI7k_YY3"
   },
   "outputs": [],
   "source": [
    "for text in Text:\n",
    "    sentiment,_ = sentiment_classifier(text,model,tokenizer)\n",
    "    print(f\"Sentiment:{sentiment:.2f}\\nText: {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUmWPzac_YY5"
   },
   "source": [
    "### Load Tweets \n",
    "\n",
    "Load the tweets from the file `\"data/lec_06_tweets_sentiment_embedding.csv\"` into a dataframe `df`.  You can use the `read_csv` function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qp-ONF1-zzt0"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KaYObwc9_YY5"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/lec_06_tweets_sentiment_embedding.csv\")\n",
    "ntweets = len(df)\n",
    "print(f\"dataframe has {ntweets} tweets\")\n",
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bho870w0_YY6"
   },
   "source": [
    "### Calculate Sentiment and Transformer Embedding of Tweets\n",
    "\n",
    "We use a for loop iterating over the rows in `df` to calculate the sentiment and transformer embedding of each tweet.  We store the sentiment in the list `Sentiment` and embeddings in the list `Embedding`.  We then add the sentiment to a column in `df`.   The speed of this code depends on if you use a CPU or GPU:\n",
    "\n",
    "CPU = 9000 tweets/hour\n",
    "\n",
    "GPU = 23000 tweets/hour\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CuyX0KUz2Lo"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zt810Mn_YY7"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "c = 0\n",
    "Sentiment = []\n",
    "Embedding = []\n",
    "for index,row in df.iterrows():  #iterate over rows of dataframe\n",
    "    c+=1\n",
    "    if c%1000==0:print(f\"Tweet {c}/{len(df)}\")  #print progres every 1000 rows\n",
    "\n",
    "    sentiment,embedding = sentiment_classifier(row.text,model,tokenizer)  #calculate sentiment and embedding of tweet\n",
    "    Sentiment.append(sentiment)  #append sentiment of tweet to Sentiment list\n",
    "    Embedding.append(embedding) #append embedding of tweet to Embedding list\n",
    "\n",
    "df['sentiment'] = Sentiment  #add sentiment column to dataframe of tweets\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgntR9i4H4WK"
   },
   "source": [
    "#### Save sentiments\n",
    "\n",
    "\n",
    "You can save the resulting dataframe to a file with the `to_csv` function.  We will set `fname_sentiment` equal to the path and filename in Google Drive where you want to save the data.  Follow the code below to mount your Google Drive into your Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78ZID24AtHGV"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lsw_7zzf4t2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/social_media_analytics/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJc3t6vIHmsZ"
   },
   "outputs": [],
   "source": [
    "#uncomment the lines below to save tweets, sentiment, and embedding to a csv file\n",
    "#fname_sentiment = \"/content/social_media_analytics/drive/MyDrive/MGT 575/data/lec_06_tweets_sentiment_v1.csv\"\n",
    "#df.to_csv(fname_sentiment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4IrYIYa_YY8"
   },
   "source": [
    "### UMAP Transformer Embedding of Tweets\n",
    "\n",
    "The tranformer turns the input text into a high dimensional vector.  This is the transformer embedding, and it is designed to separate text based on sentiment.  However, we can't really visualize such a high-dimensional object.  But no worries, UMAP will let us embed this high-dimensional vector into 2 dimensions. \n",
    "\n",
    "We apply UMAP to the transformer embedding `Embedding` to create the UMAP transformer embedding `umap_bert_embedding`.  Before doing this we have to convert `Embedding` from a list to an array.  Then we save the UMAP embedding values in `df` as `\"umap_transformer_x\"` and `\"umap_transformer_y\"`.  The dataframe is saved to the file `\"data/lec_12_tweets_sentiment_embedding.csv\"`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ja_8o-liIWqR"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyqzSjLJ_YY8"
   },
   "outputs": [],
   "source": [
    "Embedding = np.array(Embedding)\n",
    "umap_bert_embedding = umap.UMAP(n_components=2, metric='cosine').fit_transform(Embedding)\n",
    "df['umap_transformer_x'] = umap_bert_embedding[:,0]\n",
    "df['umap_transformer_y'] = umap_bert_embedding[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3xL44p9IdkN"
   },
   "source": [
    "#### Save UMAP Embeddings\n",
    "\n",
    "You can save the resulting dataframe to a file with the `to_csv` function.  Choose your Google Drive as the saving location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YKD-SYD2IZTs"
   },
   "outputs": [],
   "source": [
    "#uncomment the line below to save tweets, sentiment, and embedding to a csv file\n",
    "#fname_sentiment = \"/content/social_media_analytics/drive/MyDrive/MGT 575/data/lec_06_tweets_sentiment_embedding_v1.csv\"\n",
    "#df.to_csv(fname_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6mcKseZ_YY8"
   },
   "source": [
    "# Analyze Tweet Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1uNl91R_YY9"
   },
   "source": [
    "### Load Tweets and Sentiment\n",
    "\n",
    "Once we save the tweet sentiments, the next time we run the notebook we can just load this data instead of recalculating the sentiment.  The sentiment is in the file `\"data/lec_06_tweets_sentiment_embedding.csv\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pk3hi5iyIok0"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "318cywmy_YY9"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/lec_06_tweets_sentiment_embedding.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIJd3ZbA_YY9"
   },
   "source": [
    "### Average User Sentiment\n",
    "\n",
    "We make a bar plot of the average sentiment of each user.  We do this using the `barplot` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wcErYUIv0AQ5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "al14ZlGE_YY9"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,6))\n",
    "sns.barplot(data = df, x= 'sentiment', y = 'screen_name')\n",
    "plt.xlim([2.5,4])\n",
    "plt.xlabel(\"Screen name\",fontsize = 16)\n",
    "plt.ylabel(\"Mean sentiment\",fontsize = 16)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "plt.title(\"Sentiment vs Screen Name\",fontsize = 20)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmIdN4zC_YY-"
   },
   "source": [
    "### Sentiment Distribution per User\n",
    "\n",
    "We can make histograms of the tweet sentiment for each user.  We use a `for` loop to iterate through each screen name.\n",
    "We use the `histplot` function to make a histogram of the tweets of each user.  We also add a title to each plot so we know whose tweets they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bap4xSQa0BxE"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVpclF4O_YY-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for screen_name in df.screen_name.unique():\n",
    "    df_plot = df[df.screen_name==screen_name]\n",
    "    sns.histplot(data=df_plot, x = \"sentiment\")\n",
    "    plt.title(f\"Tweets of {screen_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtqfCrsf_YY-"
   },
   "source": [
    "### Look at Tweets with Extreme Sentiment\n",
    "\n",
    "We can select tweets of each user with very high or very low sentiment and print them out.  We do this by keeping the rows of `df` with the corresponding screen name, and then using `sort_values` to sort the user's tweets by sentiment.  We set `ascending = True` inside the `for` loop to get the most postive tweets, and set `ascending = False` to get the most negative tweets.  We set `ndisplay` equal to the number of tweets we want to print per user.\n",
    "\n",
    "To show all the funny Twitter characters, we need to use the `decode` function in the `codecs` module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-8z-tuf0FcT"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIuKP3BP_YY_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ndisplay = 3\n",
    "\n",
    "print(f\"Top {ndisplay} Most Positive Tweets per Screen Name\")\n",
    "for screen_name in df.screen_name.unique():\n",
    "    df_display = df[(df.screen_name==screen_name)].sort_values(by = ['sentiment'], ascending = False)\n",
    "    c=0\n",
    "    print(f\"\\n{screen_name}\")\n",
    "    for index,row in df_display.iterrows():\n",
    "        c+=1\n",
    "        text = codecs.decode(row.text, 'unicode_escape')\n",
    "        print(f\"\\tsentiment = {row.sentiment:.2f}: {text}\")\n",
    "        if c>=ndisplay:break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cemu8iGfGpQu"
   },
   "outputs": [],
   "source": [
    "print(f\"\\nTop {ndisplay} Most Negative Tweets per Screen Name\")\n",
    "for screen_name in df.screen_name.unique():\n",
    "    df_display = df[(df.screen_name==screen_name)].sort_values(by = ['sentiment'], ascending = True)\n",
    "    c=0\n",
    "    print(f\"\\n{screen_name}\")\n",
    "    for index,row in df_display.iterrows():\n",
    "        c+=1\n",
    "        text = codecs.decode(row.text, 'unicode_escape')\n",
    "        print(f\"\\tsentiment = {row.sentiment:.2f}: {text}\")\n",
    "        if c>=ndisplay:break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQ5SyMQU_YZA"
   },
   "source": [
    "### Plot of Retweet Count vs Sentiment\n",
    "\n",
    "To see this correlation of extreme sentiment and retweet count, we can use `barplot`.\n",
    "\n",
    " We first create a column called `star` that rounds the sentiment to the nearest integer with the `round` function.  Then we can make a plot of the retweet count versus the tweet star sentiment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6tRSoZQ_YZA"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzUaDELn_YZA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['star'] = df.sentiment.round()\n",
    "\n",
    "fig = plt.figure(figsize = (12,8))\n",
    "ax = sns.barplot(data=df, x=\"star\", y=\"retweet_count\")\n",
    "plt.xlabel(\"Sentiment\", fontsize = 16)\n",
    "plt.ylabel(\"Retweet Count\", fontsize = 16)\n",
    "plt.xticks(fontsize = 14)\n",
    "plt.yticks(fontsize = 14)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ilc8QehG_YZB"
   },
   "source": [
    "### Plot Retweet Count vs Sentiment per User\n",
    "\n",
    "We can also look at a plot for each individual user.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EENO8xxL04zu"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4ndXutl_YZB",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for screen_name in df.screen_name.unique():\n",
    "  df_plot = df[df.screen_name==screen_name]\n",
    "  fig = plt.figure(figsize=(12,6))\n",
    "  sns.barplot(data = df_plot, x = 'star', y = 'retweet_count')\n",
    "  plt.xlabel(\"Sentiment\", fontsize = 16)\n",
    "  plt.ylabel(\"Retweet Count\", fontsize = 16)\n",
    "  plt.title(f\"{screen_name}\", fontsize = 20)\n",
    "  plt.xticks(fontsize = 14)\n",
    "  plt.yticks(fontsize = 14)\n",
    "  plt.grid()\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rsVgqzq_YZB"
   },
   "source": [
    "# Visualize Transformer Embedding\n",
    "\n",
    "Now we will visulize the transformer embeddings using UMAP to see how the sentiment is distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7Tvqi3U_YZC"
   },
   "source": [
    "### Scatter Plot of UMAP Transformer Tweet Embeddings\n",
    "\n",
    "We can make a scatter plot of the UMAP transformer embeddings of the tweets.  We will color the data points by the user screen name.  We will also make another plot next to this plot where we color the data points by sentiment.  You set the column for the datapoint color with the `hue` parameter.  You can choose a color palette with the `palette` parameter.  There are many palettes you can choose from, but for discrete values like `\"screen_name\"` use the `\"bright\"` palette and for continous values like `\"sentiment\"` use the `\"vlag\"` palette.  Of course feel free to try other palettes. A complete list can be found here: https://seaborn.pydata.org/tutorial/color_palettes.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_4Ox8MZ3Kn8"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b39qO-Q3_YZC"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16,8))\n",
    "ax1 = plt.subplot(1,2,1)\n",
    "sns.scatterplot(data=df, x=\"umap_transformer_x\", y=\"umap_transformer_y\", hue=\"screen_name\", palette=\"bright\", s=5)\n",
    "plt.title(\"UMAP Transformer Embedding\")\n",
    "\n",
    "ax2 = plt.subplot(1,2,2)\n",
    "sns.scatterplot(data=df, x=\"umap_transformer_x\", y=\"umap_transformer_y\", hue=\"sentiment\", \n",
    "               palette=\"vlag\", s=15)\n",
    "plt.title(f\"UMAP Transformer Embedding\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6leyA1pfbHn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lecture06_SentimentAnalysis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
