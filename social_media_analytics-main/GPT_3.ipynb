{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uBQh-y4ZYJ-"
      },
      "source": [
        "# Lecture 23 - Twitter Bot\n",
        "In this notebook we will learn how to generate fake tweets using GPT-3.\n",
        "\n",
        "<ol type = 1>\n",
        "<li> Tweeting Fake Text</li>\n",
        "    <ol type = a>\n",
        "        <li> GPT-3</li>\n",
        "    </ol>\n",
        "<li> Perpetual Tweeting</li>  \n",
        "<li> Monday Motivation Bot </li>\n",
        "    \n",
        "<li> Pacing and Leading Bot</li>\n",
        "\n",
        "<li> Bot-Bot Conversation</li>\n",
        "\n",
        "\n",
        "</ol>\n",
        "\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzxsaepZYKC"
      },
      "source": [
        "# Clones, installs, and imports\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdFeyEF6ZYKD"
      },
      "source": [
        "## Clone GitHub Repository\n",
        "This will clone the repository to your machine.  This includes the code and data files.  Then change into the directory of the repository."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/zlisto/social_media_analytics\n",
        "\n",
        "import os\n",
        "os.chdir(\"social_media_analytics\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW-_vyO5alC1",
        "outputId": "9462ab1d-b08b-4f95-9f96-2e1f247c3063"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'social_media_analytics'...\n",
            "remote: Enumerating objects: 441, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 441 (delta 2), reused 6 (delta 2), pack-reused 435\u001b[K\n",
            "Receiving objects: 100% (441/441), 45.62 MiB | 17.41 MiB/s, done.\n",
            "Resolving deltas: 100% (234/234), done.\n",
            "Checking out files: 100% (55/55), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ3tma9qZYKE"
      },
      "source": [
        "## Install Requirements "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1qPgAZWCZYKE",
        "outputId": "f868f6e9-a8a2-4d9f-a157-ac726df4fdac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 58 kB 3.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 10.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 88 kB 7.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 56.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 50.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 43.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 163 kB 19.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 66.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 163 kB 61.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 63.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 75.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 81.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 158 kB 65.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 62.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 60.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 60.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 65.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 62.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 61.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 62.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 63.8 MB/s \n",
            "\u001b[?25h  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for openai (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdnbVa-5ZYKF"
      },
      "source": [
        "## API Keys\n",
        "\n",
        "After cloning or pulling the repository, the file `scripts/config_bot.py` will be overwritten to a blank file.  You will need to edit the file by pasting in your OpenAI API key as follows: \n",
        "\n",
        "1. `OPENAI_API_KEY = 'your OpenAI API key`\n",
        "\n",
        "We can use GPT-3 to generate text. First you need to create an account with OpenAI here: https://auth0.openai.com/u/signup?state=hKFo2SBWS3JUVEdmQmdzZXo5ckhpY3R5NEFlc2NPWWc3WHhvRqFur3VuaXZlcnNhbC1sb2dpbqN0aWTZIG9kTDB4LV83aEdnN3pRU3VUYnVZemlnZkFURFo2RDhno2NpZNkgRFJpdnNubTJNdTQyVDNLT3BxZHR3QjNOWXZpSFl6d0Q\n",
        "\n",
        "Once you have an account, copy your API key from here: https://beta.openai.com/account/api-keys\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg5OvWXiZYKG"
      },
      "source": [
        "## Import Packages\n",
        "\n",
        "The important import is from `scripts.bot` where we have the bot command methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s5BdWnfhZYKG"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from scripts.api import *\n",
        "from scripts.bot import *\n",
        "from transformers import pipeline\n",
        "import openai\n",
        "import scripts.TextAnalysis as ta\n",
        "import textwrap as tr\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import requests\n",
        "import json\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b2C-_jSZYKM"
      },
      "source": [
        "# Fake Tweets\n",
        "\n",
        "We can use the GPT-3 transformer language model to generate fake tweets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHlPfcVjZYKO"
      },
      "source": [
        "### GPT-3 Model\n",
        "\n",
        "\n",
        "\n",
        "Set all the sampling parameters.  Choose the `input_text` to start the tweet.  You can write something like `\"Write a positive tweet about Yale.\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tUOySLOaZYKO"
      },
      "outputs": [],
      "source": [
        "input_text = \"Write a positive tweet about Yale\"\n",
        "max_length = 1000\n",
        "temperature = 1\n",
        "p = 0.9\n",
        "engine =  \"text-davinci-002\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "a1qvMbfnZYKO",
        "outputId": "22e0e98d-63f7-4ffc-ce3b-7aacbff4a061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT-3 Text:\n",
            "    I'm so proud to be a part of the Yale\n",
            "community! This university has such a rich history\n",
            "and a bright future. Go Bulldogs!\n"
          ]
        }
      ],
      "source": [
        "responses = openai.Completion.create(\n",
        "  engine=engine,\n",
        "  prompt= input_text,\n",
        "  temperature=temperature,\n",
        "  max_tokens=max_length,\n",
        "  top_p=p,\n",
        "  frequency_penalty=0,\n",
        "  presence_penalty=0,\n",
        ")\n",
        "\n",
        "#print text\n",
        "text = responses['choices'][0]['text']\n",
        "print(f\"GPT-3 Text:\\n{tr.fill(text,width = 50)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK6t-W39ZYKP"
      },
      "source": [
        "# Perpetual Tweeting\n",
        "\n",
        "We use a `while` loop combined with the `sleep` function to make the bot tweet perpetually.  The bot will tweet, then sleep for a random amount of time, continuously in a loop.  We can use a language model to create the tweets.\n",
        "\n",
        "The mean sleep time is `tsleep_mean`, measured in seconds. We then add some noise to it to make it look more random to obtain the sleep time `tsleep`.  We also set `tweet_max` equal to the maximum number of tweets to generate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyEtd5i1ZYKQ"
      },
      "source": [
        "## Perpetual tweeting with GPT-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AipIX62NZYKQ"
      },
      "outputs": [],
      "source": [
        "max_length = 1000\n",
        "temperature = 1\n",
        "p = 0.9\n",
        "\n",
        "tsleep_mean = 1  #mean sleep time in seconds\n",
        "tweet_max = 2 #maxium number of tweets to generate (it costs money to make GPT-3 write a tweet)\n",
        "input_text = \"Write a funny tweet about Yale being better than Harvard.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SJxKa33kZYKQ",
        "outputId": "be432bfa-2972-4a13-ed0f-e9832b44cbd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: Ivy League schools are a joke. If you want a real education, go to Yale. Harvard is overrated.\n",
            "Sleeping for 3.55 seconds\n",
            "\n",
            "2: Yale is better than Harvard because our students are smarter, our campus is more beautiful, and our food is better.\n",
            "Sleeping for 2.53 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "c = 0\n",
        "while True:\n",
        "    c+=1\n",
        "    if c>tweet_max:break  #stop after tweet_max tweets\n",
        "    responses = openai.Completion.create(\n",
        "      engine=engine,\n",
        "      prompt= input_text,\n",
        "      temperature=temperature,\n",
        "      max_tokens=max_length,\n",
        "      top_p=p,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "    )\n",
        "\n",
        "    #print text\n",
        "    text = responses['choices'][0]['text']\n",
        "    text = text.strip()  #remove leading and trailing white spaces\n",
        "\n",
        "    print(f\"{c}: {text}\")\n",
        "    \n",
        "    tsleep = tsleep_mean + np.random.uniform(low=0.0, high=3)\n",
        "    print(f\"Sleeping for {tsleep:.2f} seconds\\n\")\n",
        "    time.sleep(tsleep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhZT8cT-ZYKQ"
      },
      "source": [
        "# Monday Motivation Bot\n",
        "\n",
        "This bot will tweet something about #MondayMotivation each Monday."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IZAHGqEoZYKR"
      },
      "outputs": [],
      "source": [
        "max_length = 1000\n",
        "temperature = 1\n",
        "p = 0.9\n",
        "\n",
        "tsleep_mean = 1  #mean sleep time in seconds\n",
        "tweet_max = 2\n",
        "input_text = \"Write a clever tweet using the hashtag #MondayMotivation.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qQs-xEYsZYKR",
        "outputId": "50300ad4-b4a9-4f84-e54d-3e7571b4b7b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: #MondayMotivation is a great day to get things done and check things off your to-do list!\n",
            "Sleeping for 2.72 seconds\n",
            "\n",
            "2: #MondayMotivation: Don't let the haters get you down.\n",
            "Sleeping for 1.91 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "c = 0\n",
        "while True:\n",
        "    c+=1\n",
        "    if c>tweet_max: break  #stop after tweet_max tweets\n",
        "    responses = openai.Completion.create(\n",
        "      engine=engine,\n",
        "      prompt= input_text,\n",
        "      temperature=temperature,\n",
        "      max_tokens=max_length,\n",
        "      top_p=p,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "    )\n",
        "\n",
        "    #print text\n",
        "    text = responses['choices'][0]['text']\n",
        "    text = text.strip()\n",
        "    \n",
        "    print(f\"{c}: {text}\")\n",
        "    \n",
        "    \n",
        "    tsleep = tsleep_mean + np.random.uniform(low=0.0, high=2)\n",
        "    print(f\"Sleeping for {tsleep:.2f} seconds\\n\")\n",
        "    time.sleep(tsleep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi0jLXNcZYKR"
      },
      "source": [
        "# Pacing and Leading Bot\n",
        "\n",
        "This bot will implement a pacing and leading policy.  Set `sentiments` equal to the sentiment for each tweet as a list and set `topic` equal to the topic of your perusasion.  The sentiment should be a number between 0 and 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "I_bgIm62ZYKR",
        "outputId": "8d5550e9-37c6-4866-ed01-8bfd487ef7e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiments = [0, 1, 2, 3, 4, 5]\n"
          ]
        }
      ],
      "source": [
        "max_length = 1000\n",
        "temperature = 1\n",
        "p = 0.9\n",
        "\n",
        "tsleep_mean = 1  #mean sleep time in seconds\n",
        "topic = \"Kanye West\"\n",
        "sentiments = list(range(0,6))\n",
        "\n",
        "print(f\"sentiments = {sentiments}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vvypuQMOZYKR",
        "outputId": "30f3a396-de04-4e0a-fce6-198c0f40da23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pacing and leading about Kanye West\n",
            "Tweet 1: sentiment = 0/5.0: Kanye West is the worst musician of all time.\n",
            "Sleeping for 2.33 seconds\n",
            "\n",
            "Tweet 2: sentiment = 1/5.0: Kanye is the worst. He's a washed up rapper who is only relevant because of his wife.\n",
            "Sleeping for 1.18 seconds\n",
            "\n",
            "Tweet 3: sentiment = 2/5.0: Kanye West is one of the most controversial figures in music today.\n",
            "Sleeping for 2.51 seconds\n",
            "\n",
            "Tweet 4: sentiment = 3/5.0: Kanye West is a controversial figure, but there's no denying his talent.\n",
            "Sleeping for 1.02 seconds\n",
            "\n",
            "Tweet 5: sentiment = 4/5.0: Kanye West is one of the most innovative and exciting artists of our generation.\n",
            "Sleeping for 2.32 seconds\n",
            "\n",
            "Tweet 6: sentiment = 5/5.0: Kanye West is a musical genius and one of the best rappers of all time.\n",
            "Sleeping for 2.71 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Pacing and leading about {topic}\")\n",
        "c = 0\n",
        "for sentiment in sentiments:\n",
        "    c+=1\n",
        "    input_text = f\"Write a tweet with sentiment {sentiment}/5.0 about {topic}.\"\n",
        "\n",
        "    responses = openai.Completion.create(\n",
        "      engine=engine,\n",
        "      prompt= input_text,\n",
        "      temperature=temperature,\n",
        "      max_tokens=max_length,\n",
        "      top_p=p,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0,\n",
        "    )\n",
        "\n",
        "    #print text\n",
        "    text = responses['choices'][0]['text']\n",
        "    text = text.strip()\n",
        "    \n",
        "    print(f\"Tweet {c}: sentiment = {sentiment}/5.0: {text}\")\n",
        "      \n",
        "    \n",
        "    tsleep = tsleep_mean + np.random.uniform(low=0.0, high=2)\n",
        "    print(f\"Sleeping for {tsleep:.2f} seconds\\n\")\n",
        "    time.sleep(tsleep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk9ZoFBbZYKS"
      },
      "source": [
        "# Bot Conversation\n",
        "\n",
        "We can make GPT-3 talk to itself.  Name `speaker0` and `speaker1` and describe them in  `desc0` and `desc1`.  Then set the first messages between them as `text0` and `text1`.  Set how many messages you want to generate with `msg_max`.  Then run the code and see what the spekaers have to say to each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CWftvmg2ZYKS"
      },
      "outputs": [],
      "source": [
        "msg_max = 6\n",
        "speaker0 = \"Goku\"\n",
        "speaker1 = \"Vegeta\"\n",
        "\n",
        "context = f\"The following is a conversation between {speaker0} and {speaker1}.\"\n",
        "\n",
        "desc0 = f\"{speaker0} is from Dragonball Z and is kind and funny.\"\n",
        "desc1 = f\"{speaker1} is from Dragonball Z and is angry and jealous.\"\n",
        "\n",
        "text0 = f\"{speaker0}: Hey Vegeta!\"\n",
        "text1 = f\"{speaker1}: Go away Goku.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mHvWuZ1JZYKS",
        "outputId": "a94fe66c-67d5-4c56-b05d-4b7111d39b32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a conversation between Goku and Vegeta.\n",
            "Goku is from Dragonball Z and is kind and funny.\n",
            "Vegeta is from Dragonball Z and is angry and jealous.\n",
            "\n",
            "Goku: Hey Vegeta!\n",
            "Vegeta: Go away Goku.\n",
            "Goku: What's wrong?\n",
            "Vegeta: I'm angry and jealous.\n",
            "Vegeta: I'm angry because I can't beat you and I'm jealous because you're stronger than me.\n",
            "Goku: Oh, I see.\n",
            "Goku:Well, I guess I can understand that.\n",
            "Vegeta:Shut up!\n",
            "Vegeta: I don't want your sympathy!\n",
            "Goku: Well, I'm sorry if you don't want my sympathy, but I do understand how you feel. It can be tough to not be the strongest and to always be second best.\n",
            "Vegeta: Fine, whatever. Just leave me alone.\n"
          ]
        }
      ],
      "source": [
        "input_text = f\"{context}\\n{desc0}\\n{desc1}\\n\\n{text0}\\n{text1}\"\n",
        "\n",
        "print(input_text)\n",
        "\n",
        "for i in range(msg_max):\n",
        "    if i%2==0:\n",
        "        speaker = speaker0\n",
        "    elif i%2==1:\n",
        "        speaker = speaker1\n",
        "\n",
        "    input_text+=f\"\\n{speaker}:\"\n",
        "    responses = openai.Completion.create(\n",
        "      engine=engine,\n",
        "      prompt= input_text,\n",
        "      temperature=temperature,\n",
        "      max_tokens=max_length,\n",
        "      top_p=p,\n",
        "      frequency_penalty=0,\n",
        "      presence_penalty=0.4,\n",
        "    )   \n",
        "    text = responses['choices'][0]['text']\n",
        "    text = text.strip()\n",
        "    #text +=\"\\n\"\n",
        "    input_text += text\n",
        "    print(f\"{speaker}: {text}\")\n",
        "    time.sleep(2)\n",
        "\n",
        "\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aaj3eWaze63k"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}