{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUOKSuOWzt3u"
   },
   "source": [
    "# Homework 8\n",
    "\n",
    "This notebook provides some skeleton code to get you started on the homework. Add in your own code and markdown cells to answer the homework questions. If you want to submit the notebook as a PDF, make sure your code and markdowns are clear and concise to make grading easy for the TAs.\n",
    "\n",
    "This notebook can be opened in Colab \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zlisto/social_media_analytics/blob/main/HW8.ipynb)\n",
    "\n",
    "\n",
    "Before starting, select \"Runtime->Factory reset runtime\" to start with your directories and environment in the base state.\n",
    "\n",
    "If you want to save changes to the notebook, select \"File->Save a copy in Drive\" from the top menu in Colab. This will save the notebook in your Google Drive.\n",
    "\n",
    "For all plots, make sure your axes have nice labels with easy to read fontsizes, otherwise points will be deducted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZ2fpQvAzt30"
   },
   "source": [
    "# Clones, Installs, and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGDakYwZxTOv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHBFAlG3zt33"
   },
   "source": [
    "# Problem 1. (52 points) Compare Decoding Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ik7KnaU7zt33"
   },
   "source": [
    "## 1. (4 points) Load GPT-2 Model\n",
    "\n",
    "Load the GPT-2 transformer using the `pipeline` function and configure it for `\"text-generation\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-mMG4GIzt34"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4gvHwzxzt35"
   },
   "source": [
    "## 2. (8 points) Greedy Search\n",
    "\n",
    "Set `input_text = \"Hi my name is\"` and use greedy search to generate a text of maximum length 50 using your GPT-2 `generator`.  Print out your results using the `display_text` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfWta-2_zt36"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh9m1z3Yzt37"
   },
   "source": [
    "## 3. (8 points) Beam Search\n",
    "\n",
    "Set `input_text = \"Hi my name is\"` and use beam search with 10 beams to generate a text of maximum length 50 using your GPT-2 `generator`.  Print out your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJC6qJHIzt37"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQHFWTnFzt38"
   },
   "source": [
    "## 4. (8 points) Beam versus Greedy Search\n",
    "\n",
    "Repeat Problem 1.3, but now use only 1 beam.  Print our your results.  You might notice this looks like greedy search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yy0ShTcpzt38"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8CIbt1Kzt38"
   },
   "source": [
    "## 5. (10 points) Sampling with Temperature\n",
    "\n",
    "Set `input_text = \"Hi my name is\"` and use sampling with temperature to generate 5 texts of maximum length 50 using your GPT-2 `generator`.  Set the temperature to 0.5, 1, 2, and 3 (a `for` loop might be useful here).    Print out your results (the temperature and the generated text ,make sure the TAs can understand which text goes with which temperature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSkkNpS6zt39",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTlghNnezt39"
   },
   "source": [
    "## 7. (10 points) Top-P Sampling\n",
    "\n",
    "Set `input_text = \"Hi my name is\"` and use top-p sampling to generate 5 texts of maximum length 50 using your GPT-2 `generator`.  Set p equal to  0.1, 0.5, and 0.9 (a `for` loop might be useful here).   Print out your results (the p and the generated text ,make sure the TAs can understand which text goes with which p)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CG4cqUOTzt3-",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GT26hz2czt3-"
   },
   "source": [
    "## 8. (4 points) P Effect\n",
    "\n",
    "Describe the qualitative difference in the generated text as `p` increases.  Discuss both the variety of the text and the coherence of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peZf1IJUzt3-"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMPLJonkzt3-"
   },
   "source": [
    "# Problem 2. (28 points) Fine-Tuning on Tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAAjjL36zt3_"
   },
   "source": [
    "## 1. (4 points) Load Fine-Tuned GPT-2 Model\n",
    "\n",
    "Set `screen_name = \"KimKardashian\"`. Load the fine-tuned GPT-2 transformer for this user with the `pipeline` function and configure it for `\"text-generation\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTXsSEA4zt3_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6hULSZyzt3_"
   },
   "source": [
    "## 2. (10 points) Generate Fake Tweets with Top-P Sampling with Temperature\n",
    "\n",
    "Set `input_text = \"Today is going to be\"` and use top-p sampling to generate 5 texts of maximum length 50 using your fine-tuned GPT-2 `generator`.  Set `p=0.9` and `temperature = 0.8`.  Print your output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sQrQe55zt3_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hciuBy31zt4A"
   },
   "source": [
    "## 2. (10 points) Generate Fake Tweets with Top-P Sampling with Higher Temperature\n",
    "\n",
    "Set `input_text = \"Today is going to be\"` and use top-p sampling to generate 5 texts of maximum length 50 using your fine-tuned GPT-2 `generator`.  Set `p=0.9` and `temperature = 1.8`.  Print your output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fl0GmsHyzt4A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osuazr_tzt4A"
   },
   "source": [
    "## (4 points) Comparing Temperatures\n",
    "\n",
    "How many of your fake tweets at a temperature of 0.8 sound real?\n",
    "\n",
    "How many of your fake tweets at a temperature of 1.8 sound real?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGvTxiDPzt4A"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AENMMzwQzt4A"
   },
   "source": [
    "# 3. Problem 3. Language Model Probability Distribution (18 points) \n",
    "\n",
    "In this problem we will compare GPT-2 with the fine-tuned GPT-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGbOPKQBzt4B"
   },
   "source": [
    "## 1. (6 points) Sample Words from GPT-2\n",
    "\n",
    "Set `input_text = \"Hi my name is Kim\"`.  Generate 1000 samples of the next word in this text using GPT-2.  Use the `sample_words` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2godWhGszt4B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3EYeNtQzt4B"
   },
   "source": [
    "## 2. (6 points) Sample Words from Fine-Tuned GPT-2\n",
    "\n",
    "Set `input_text = \"Hi my name is Kim\"`.  Generate 1000 samples of the next word in this text using the fine-tuned GPT-2.  Use the `sample_words` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mMzek3Izt4B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbZcqa5kzt4B"
   },
   "source": [
    "## 3. (4 points) Histograms of Word Distributions for GPT-2 and Fined Tuned GPT-2\n",
    "\n",
    "Make a histogram of the top 10 most frequent sampled words for GPT-2 and fine tuned GPT-2.  Label axes nicely.  Also, make sure to title the fine-tuned GPT-2 histogram with the screen name of user whose tweets were used to fine-tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZMwlb-0Czt4B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLWxh1lxzt4C"
   },
   "source": [
    "## 4. (2 points) Fine-Tuning Effect\n",
    "\n",
    "Describe some of the differences among the most common words between GPT-2 and fine-tuned GPT-2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nByvt1Nwzt4C"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW7.ipynb",
   "provenance": [
    {
     "file_id": "1q2c0Pz2CUsNoNVw3q13NLWbTVyZB7SWB",
     "timestamp": 1649313759987
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
